<!DOCTYPE html>
<html lang="en">

<head>
     
    <meta charset="utf-8">
     
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
     
    <meta name="viewport" content="width=device-width, initial-scale=1">

      <title>Freyja - Join Quality Measure</title>

     
    <link rel="stylesheet" href="freyja.css">
     
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css"
        integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
     
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
     
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-Piv4xVNRyMGpqkS2by6br4gNJ7DXjqk09RmUpJ8jgGtD7zP9yug3goQfGII0yAns"
        crossorigin="anonymous"></script>
     
    <script src="https://kit.fontawesome.com/92dab46df1.js" crossorigin="anonymous"></script>
</head>

<body>

    <div class="container">

          <nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light">
            <a class="navbar-brand" href="index.html"><span
                    style="font-variant: small-caps; font-size: 1.5em;">Freyja</span></a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
                aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                 <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse">
                 <ul class="navbar-nav mr-auto">
                    <li class="nav-item"><a class="nav-link" href="index.html#people"
                            style="margin-top: 8px; font-size: 1.2em;">People</a></li>
                    <li class="nav-item"><a class="nav-link" href="index.html#publications"
                            style="margin-top: 8px; font-size: 1.2em;">Publications</a></li>
                    <li class="nav-item"><a class="nav-link" href="index.html#resources"
                            style="margin-top: 8px; font-size: 1.2em;">Resources</a></li>
                    <li class="nav-item"><a class="nav-link" href="index.html#reproducibility"
                            style="margin-top: 8px; font-size: 1.2em;">Reproducibility</a></li>
                    <li class="nav-item"><a class="nav-link" href="index.html#demo"
                            style="margin-top: 8px; font-size: 1.2em;">Demo</a></li>
                     
                </ul>
            </div>
             
        </nav>

          <br>

          <h2 style="text-align: center;">Join Quality Metric</h2>

        <p style="font-size: 0.9em; color: grey;">
            Link to <a target="_blank"
                href="https://github.com/dtim-upc/FREYJA/blob/main/experiments/notebooks/join_quality_measure.ipynb">notebook</a>.
            The notebook includes all the necessary code to create the data and generate the plots. Nonetheless, an
            iteration of this data is already contained <a target="_blank"
                href="https://mydisk.cs.upc.edu/s/5n8jRDm4neF5HfR">here</a>, under the <i>k_justification</i> folder.
        </p>

        <p>
            Here we focus on developing a <b>novel join quality measure</b> specifically for <b>data lakes</b>, which
            combines a syntactic set overlap computation (<b>Multiset Jaccard, MJ</b>) and the <b>Cardinality Proportion (K)</b>
            to lightly indicate semantic relatedness:
        </p>

        <h5>Preparing the data</h5>
        <p class="text-justify">
            First, we define functions to compute all the necessary metrics (Multiset Jaccard, Jaccard, containment, and
            the cardinality proportion). This allows us to compare the basic set-overlap metrics and discern which is
            best (and later combine them with the cardinality proportion). We start by loading the ground truth, which
            is a dataframe of joins (pairs of attributes from different datasets) with the following measurements:
            containment (C), cardinality proportion (K), Jaccard (J) and multiset Jaccard (MJ) scores. MJ and K will be
            used to derive the final metric, whereas C and J will be used as baselines to test the effectiveness of MJ.

            Additionally, each join is assigned a type of relationship (semantic or syntactic). A join is <b>syntactic</b>
            if containment > 0.1. A join is <b>semantic</b> if, on top of being syntactic, the columns have a meaningful
            relationship (e.g., both columns represent countries). This assignment has been done manually.
        </p>

        <h5>Initial experiments</h5>
        <p class="text-justify">
            We conduct a series of initial experiments that showcase that <b>Multiset Jaccard is better suited for
            joinability detection</b> than both containment and Jaccard, mainly by checking how well these metrics can
            separate semantic from syntactic joins. This is likely due to Multiset Jaccard taking into consideration
            multisets, which provides an advantage in denormalized scenarios, such as data lakes, where repeated values
            are common. Nonetheless, the results are not good enough, so we need a way to improve them.
        </p>
        <p class="text-justify">
            We start by ordering the joins in three different "rankings," one per each set overlap metric, with higher
            values placed at the top. Then, we separate each ranking into its four quartiles and measure the proportion
            of semantic and syntactic joins in each. Ideally, for quartiles 3 and 4 (representing the top 50% of joins),
            they should have a higher proportion of semantic joins than quartiles 1 and 2. Note that the ground truth
            contains more syntactic than semantic joins.
        </p>
        <p>
            <b>Result:</b> Containment is clearly worse than both Jaccard metrics in developing this separation. Multiset
            Jaccard seems to make the distribution slightly better than Jaccard, although the evidence is small.
        <div style="text-align: center;">
            <img src="img/metric/barplot.png"
                alt="Comparison of semantic and syntactic join proportions across quartiles for Containment, Jaccard, and Multiset Jaccard"
                style="width: 100%; height: auto;">
            <p style="font-size: 0.9em; margin-top: 5px;">
                <strong>Figure 1.</strong> Comparison of semantic and syntactic join proportions across quartiles for
                Containment, Jaccard, and Multiset Jaccard.
            </p>
        </div>
        </p>
        <p class="text-justify">
            Since a real user only considers the top $k$ joins, we now focus on the <b>Precision at $k$ (P@k)</b> metric:
            how many of the top $k$ joins are semantic? We set $k$ to 100, which is far more than a realistic end user
            would consider.
        </p>
        <p class="text-justify">
            <b>Result:</b> We can observe that <b>Multiset Jaccard is clearly better at placing semantic joins in the top
            spots</b>. Both containment and Jaccard place syntactic joins in the first few positions, meaning the first
            joins given to the user would be false positives. We have a clear argument to prefer Multiset Jaccard.
        <div style="text-align: center;">
            <img src="img/metric/set_overlap_comparison.png"
                alt="Precision at k (P@k) for Containment, Jaccard, and Multiset Jaccard, showing performance for top joins"
                style="width: 80%; height: auto;">
            <p style="font-size: 0.9em; margin-top: 5px;">
                <strong>Figure 2.</strong> Precision at $k$ (P@k) for Containment, Jaccard, and Multiset Jaccard,
                showing performance for top joins.
            </p>
        </div>
        </p>

        <h5>Relevance of K</h5>
        <p class="text-justify">
            We move to testing the <b>relevance of the cardinality proportion (K)</b> for join detection. We observe that
            the proportion of K for syntactic and semantic joins varies considerably, which indicates that <b>K can be
            used as a complimentary and lightweight assessment of join quality</b> in the context of data lakes.
        </p>
        <p class="text-justify">
            This variation can be seen by comparing the distribution of K for semantic and syntactic joins.
        </p>
        <div style="text-align: center;">
            <img src="img/metric/barplot_k.png"
                alt="Distribution comparison of the Cardinality Proportion (K) for semantic and syntactic joins"
                style="width: 60%; height: auto;">
            <p style="font-size: 0.9em; margin-top: 5px;">
                <strong>Figure 3.</strong> Distribution comparison of the Cardinality Proportion (K) for semantic and
                syntactic joins.
            </p>
        </div>

        <div style="text-align: center;">
            <img src="img/metric/violinplot.png"
                alt="Violin plot showing the distribution of the Cardinality Proportion (K) for different join types"
                style="width: 100%; height: auto;">
            <p style="font-size: 0.9em; margin-top: 5px;">
                <strong>Figure 4.</strong> Violin plot showing the distribution of the Cardinality Proportion (K) for
                different join types.
            </p>
        </div>

        <h5>Multiclass metric</h5>
        <p class="text-justify">
            Once we have decided on the use of <b>MJ and K</b>, we create a conjoined metric that takes both measurements
            into account. By plotting the scores for MJ and K obtained by the ground truth, we can observe a <b>positive
            correlation</b> between the values of the metrics and the semantic category of the joins: the higher the
            values of MJ and K, the more likely a join is semantic.
        </p>
        <p class="text-justify">
            Hence, we define a <b>discrete multiclass metric</b> that maps the ground truth instances onto four (by
            default, but there might be many levels) quality categories, ensuring that the "higher" buckets contain a
            greater proportion of semantic joins compared to syntactic joins. This is not meant to define 100% accurate
            thresholds, but rather to capture general trends. The selected function is arbitrary, as there is an
            infinite amount of functions to try; our proposal is a reasonable choice among many. We plot the result of
            applying such function over our ground truth.
        </p>

        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 25px; margin: 30px auto; max-width: 1200px;">

            <div style="text-align: center;">
                <img src="img/metric/scatter_binary.png"
                    alt="Scatter plot of Multiset Jaccard (MJ) versus Cardinality Proportion (K) categorized by binary join type"
                    style="width: 100%; height: auto;">
                <p style="font-size: 0.9em; margin-top: 5px;">
                    <strong>Figure 5.</strong> Scatter plot of Multiset Jaccard (MJ) versus Cardinality Proportion (K)
                    categorized by binary join type.
                </p>
            </div>

            <div style="text-align: center;">
                <img src="img/metric/scatter_4.png"
                    alt="Scatter plot of Multiset Jaccard (MJ) versus Cardinality Proportion (K) using the four quality multiclass metric"
                    style="width: 100%; height: auto;">
                <p style="font-size: 0.9em; margin-top: 5px;">
                    <strong>Figure 6.</strong> Scatter plot of Multiset Jaccard (MJ) versus Cardinality Proportion (K)
                    using the four quality multiclass metric.
                </p>
            </div>
        </div>

        <p class="text-justify">
            Finally, we transform the multi-class metric to a <b>continuous one</b> to allow for rankings. To do so, we fit
            the discrete metric to a continuous function by computing the <b>Wasserstein distance</b> between the discrete
            function and continuous functions generated from Gaussian distributions with a series of fixed parameters.
            The final continuous function is the one that minimized the Wasserstein distance for both K and MJ. We also
            showcase that $L = 4$ (four buckets) is the value that minimizes the distances, hence it is the value
            selected for the final function.
        </p>

        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 25px; margin: 30px auto; max-width: 1200px;">

            <div style="text-align: center;">
                <img src="img/metric/fitting_k.png"
                    alt="Continuous fitting of the discrete metric for the Cardinality Proportion (K)"
                    style="width: 100%; height: auto;">
                <p style="font-size: 0.9em; margin-top: 5px;">
                    <strong>Figure 7.</strong> Continuous fitting of the discrete metric for the Cardinality Proportion
                    (K).
                </p>
            </div>

            <div style="text-align: center;">
                <img src="img/metric/fitting_mj.png"
                    alt="Continuous fitting of the discrete metric for Multiset Jaccard (MJ)"
                    style="width: 100%; height: auto;">
                <p style="font-size: 0.9em; margin-top: 5px;">
                    <strong>Figure 8.</strong> Continuous fitting of the discrete metric for Multiset Jaccard (MJ).
                </p>
            </div>
        </div>

        <div style="text-align: center;">
            <img src="img/metric/wass.png" alt="Wasserstein distance as a function of the number of classes L"
                style="width: 50%; height: auto;">
            <p style="font-size: 0.9em; margin-top: 5px;">
                <strong>Figure 9.</strong> Wasserstein distance as a function of the number of classes $L$.
            </p>
        </div>


        <h5>Strictness</h5>
        <p class="text-justify">
            Finally, to facilitate reproducibility, we add a <b>strictness measurement</b> to the metric, which can alter
            its behavior to be more restrictive. The higher the strictness, the more restrictive the scoring will be.
            That is, a higher strictness value implies that higher scores of MJ and K are necessary to achieve a high
            join quality score. We define three levels: <b>relaxed</b>, <b>balanced</b> (which is the one we build the final
            model with), and <b>strict</b>. As we want to provide a single model to prevent cumbersome configuration for
            the user, we build the final model with the balanced strictness value.
        </p>


        <div style="text-align: center;">
            <img src="img/metric/strictness.png"
                alt="Comparison of the final metric's performance under different strictness levels (relaxed, balanced, strict)"
                style="width: 100%; height: auto;">
            <p style="font-size: 0.9em; margin-top: 5px;">
                <strong>Figure 10.</strong> Comparison of the final metric's performance under different strictness
                levels (relaxed, balanced, strict).
            </p>
        </div>


         

          <p><a href="index.html">&larr; Back to main page</a></p>

          <p style="font-size: small; color: #888;">Last updated: 2025/10/14</p>
    </div>

</body>

</html>