<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Freyja - Models</title>

  <link rel="stylesheet" href="freyja.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-Piv4xVNRyMGpqkS2by6br4gNJ7DXjqk09RmUpJ8jgGtD7zP9yug3goQfGII0yAns" crossorigin="anonymous"></script>
  <script src="https://kit.fontawesome.com/92dab46df1.js" crossorigin="anonymous"></script>
</head>

<body>

<div class="container">

  <nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light">
    <a class="navbar-brand" href="index.html"><span style="font-variant: small-caps; font-size: 1.5em;">Freyja</span></a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item"><a class="nav-link" href="index.html#people" style="margin-top: 8px; font-size: 1.2em;">People</a></li>
        <li class="nav-item"><a class="nav-link" href="index.html#publications" style="margin-top: 8px; font-size: 1.2em;">Publications</a></li>
        <li class="nav-item"><a class="nav-link" href="index.html#resources" style="margin-top: 8px; font-size: 1.2em;">Resources</a></li>
        <li class="nav-item"><a class="nav-link" href="index.html#reproducibility" style="margin-top: 8px; font-size: 1.2em;">Reproducibility</a></li>
        <li class="nav-item"><a class="nav-link" href="index.html#demo" style="margin-top: 8px; font-size: 1.2em;">Demo</a></li>
      </ul>
    </div>
  </nav>

  <br>

  <h2 style="text-align: center;">Model development</h2>

  <p style="font-size: 0.9em; color: grey;">
    Link to <a target="_blank" href="https://github.com/dtim-upc/FREYJA/blob/main/experiments/notebooks/models.ipynb">notebook</a>. The notebook includes all the necessary code to create the data that we need and generate the plots. Nonetheless, an iteration of this data is already contained <a target="_blank" href="https://mydisk.cs.upc.edu/s/5n8jRDm4neF5HfR">here</a>, under the <i>k_justification</i> folder.
  </p>

    <p class="text-justify">
      This notebooks presents a long process of <b>defining and testing several models</b> before selecting one of them to be part of the final system. We define four base models: 
      <ol>
        <li><b>All metrics</b> (<i>all</i>). This model contains all the original metrics of the profiles (over 60), likely to overfit and contain a lot of redundancy and unnecessary information</li>
        <li><b>All metrics + lightweight feature selection</b> (<i>all_fs_simple</i>). We apply a simple feature selection process to remove irrelevant model features ad hoc via an embedded method. The goal is to just remove those features that are clearly redundant without taking too much time and not altering the core of the model.</li>
        <li><b>All metrics + in-depth feature selection</b> (<i>all_fs_deep</i>). For this model we test a multi-layer (several filter, wrapper and embedded tasks) and aggresive feature selection process, whose goal is to obtain a far superior subset of metrics. The details of the feature selection are described in the following paragraph.</li>
        <li><b>Custom set of metrics</b> (<i>custom</i>). For the final model we select a custom subset of metrics, according to previous experimentation.</li>
      </ol>
    
    </p>
    <p class="text-justify">
        Feature selection is a standard process in the field of data science and machine learning, as most features in a given dataset tend to be redundant and not useful. Hence, reducing the number of features to the most relevant reduces the complexity of the model and prevents overfitting to superfluous variables. Our initial set of features comprised 65 different metrics, so the likeliness that many of these features were irrelevant was high, hence the introduction of a feature selection process. Our goal is to keep only those very meaningful features that will surely maintain their relevance when obtained in external benchmarks.
    </p>
    <p class="text-justify">
        To improve the robustness of our approach, we conducted a new, more thorough feature selection process based on several layers:
        <ol class="text-justify">
            <li>
                We employ <b>filter methods</b> (extremely fast, based on relationships between the attributes) to remove clearly unhelpful features without requiring model intervention. This layer encompassed three substeps:
                <ul class="text-justify">
                    <li>
                        Variability analysis: features with less than 1% of value variability are removed, as such a low spectrum of values is highly unlikely to provide the model with useful patterns to make predictions.
                    </li>
                    <li>
                        Redundancy analysis: we remove highly redundant features (above 90% correlation).
                    </li>
                    <li>
                        Relevancy analysis: we remove features that have less than 0.01 Mutual Information (MI) score w.r.t. the target variable, as the probability that such small correlations provide meaningful insight is very low.
                    </li>
                </ul>
            </li>
            <li>
                We used the inherent capacity of our ensemble-based models (i.e. <b>embedded approach</b>) to assign relevance scores to each feature based on how much each contributed to the prediction. Features under a given “relevancy threshold” were pruned. This provides a quick filter to remove features that the model deems to be not useful. More precisely, we remove those features whose relevance w.r.t. the model is less than 0.0001%
            </li>
            <li>
                We employ a costly <b>wrapper method</b> to prune out more irrelevant features, now based on an exhaustive cross-validation task. This can be done at this stage because we removed more than half of the original features, so even though the overall wrapper process is still costly, it is feasible to compute. More precisely, we employed a Recursive Feature Elimination with Cross Validation (RFECV), with a 5-Fold cross-validation and removing a single feature each step.
            </li>
        </ol>
    </p>

    </p>
    <p>
      These models are instantiated with up to <b>17 base regressors</b>. We test all the possible combinations with our benchmark, selecting a subset of the best 20 models based on a conjoined analysis over RMSE, MAE and MedAE scores. This subset of models is tested on all the remaining benchmarks, in order to ensure that we have not commited a mistake and some of the underperforming models in the creation stage then outperforms the others when generalizing to other benchmarks. The results, however, are clear, and the <b>Gradient Boosting regressor, with custom features</b> is the model that clearly performs best, both in our benchmark as well as the other's.
    </p>

    <p>
        To better illustrate this, we took the best performing model from the training stage and tested on the external benchmarks.
    </p>


    <h6>Top 5 Model Rankings Santos Small</h6>
    <table width="100%">
        <thead>
            <tr>
                <th>Rank</th>
                <th>Model</th>
                <th>Avg Precision</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>1</td><td>gradient_boosting_custom</td><td>0.9586</td></tr>
            <tr><td>2</td><td>gradient_boosting_all</td><td>0.9490</td></tr>
            <tr><td>3</td><td>gradient_boosting_all_fs_simple</td><td>0.9443</td></tr>
            <tr><td>4</td><td>extra_trees_all</td><td>0.9386</td></tr>
            <tr><td>5</td><td>extra_trees_all_fs_simple</td><td>0.9369</td></tr>
        </tbody>
    </table>

    <br>
    <h6>Top 5 Model Rankings TUS Small</h6>
    <table width="100%">
        <thead>
            <tr>
                <th>Rank</th>
                <th>Model</th>
                <th>Avg Precision</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>1</td><td><b>gradient_boosting_custom</b></td><td>0.8875</td></tr>
            <tr><td>2</td><td>catboost_all_fs_deep</td><td>0.8676</td></tr>
            <tr><td>3</td><td>catboost_custom</td><td>0.8425</td></tr>
            <tr><td>4</td><td>gradient_boosting_all</td><td>0.8356</td></tr>
            <tr><td>5</td><td>catboost_all_fs_simple</td><td>0.8272</td></tr>
        </tbody>
    </table>
    <br>
    <h6>Top 5 Model Rankings TUS Big</h6>
    <table width="100%">
        <thead>
            <tr>
                <th>Rank</th>
                <th>Model</th>
                <th>Avg Precision</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>1</td><td><b>gradient_boosting_custom</b></td><td>0.9267</td></tr>
            <tr><td>2</td><td>gradient_boosting_all</td><td>0.9155</td></tr>
            <tr><td>3</td><td>gradient_boosting_all_fs_simple</td><td>0.9143</td></tr>
            <tr><td>4</td><td>extra_trees_all</td><td>0.9012</td></tr>
            <tr><td>5</td><td>extra_trees_all_fs_deep</td><td>0.9011</td></tr>
        </tbody>
    </table>
    <br>
    <h6>Top 5 Model Rankings D3L</h6>
    <table width="100%">
        <thead>
            <tr>
                <th>Rank</th>
                <th>Model</th>
                <th>Avg Precision</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>1</td><td><b>gradient_boosting_custom</b></td><td>0.7788</td></tr>
            <tr><td>2</td><td>catboost_custom</td><td>0.6327</td></tr>
            <tr><td>3</td><td>catboost_all_fs_deep</td><td>0.6283</td></tr>
            <tr><td>4</td><td>xgboosting_custom</td><td>0.6259</td></tr>
            <tr><td>5</td><td>gradient_boosting_all_fs_simple</td><td>0.6054</td></tr>
        </tbody>
    </table>

    <h6>Top 5 Model Rankings Freyja</h6>
    <table width="100%">
        <thead>
            <tr>
                <th>Rank</th>
                <th>Model</th>
                <th>Avg Precision</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>1</td><td><b>gradient_boosting_custom</b></td><td>0.9624</td></tr>
            <tr><td>2</td><td>gradient_boosting_all</td><td>0.9387</td></tr>
            <tr><td>3</td><td>gradient_boosting_all_fs_simple</td><td>0.9363</td></tr>
            <tr><td>4</td><td>gradient_boosting_all_fs_deep</td><td>0.9213</td></tr>
            <tr><td>5</td><td>xgboosting_custom</td><td>0.8898</td></tr>
        </tbody>
    </table>

    <h6>Top 5 Model Rankings OM CG</h6>
    <table width="100%">
        <thead>
            <tr>
                <th>Rank</th>
                <th>Model</th>
                <th>Avg Precision</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>1</td><td><b>gradient_boosting_custom</b></td><td>0.5763</td></tr>
            <tr><td>2</td><td>catboost_custom</td><td>0.5552</td></tr>
            <tr><td>3</td><td>extra_trees_all_fs_deep</td><td>0.5526</td></tr>
            <tr><td>4</td><td>extra_trees_all</td><td>0.5483</td></tr>
            <tr><td>5</td><td>extra_trees_custom</td><td>0.5448</td></tr>
        </tbody>
    </table>

    <h6>Top 5 Model Rankings OM CR</h6>
    <table width="100%">
        <thead>
            <tr>
                <th>Rank</th>
                <th>Model</th>
                <th>Avg Precision</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>1</td><td><b>gradient_boosting_custom</b></td><td>0.5996</td></tr>
            <tr><td>2</td><td>extra_trees_all</td><td>0.5490</td></tr>
            <tr><td>3</td><td>extra_trees_all_fs_simple</td><td>0.5444</td></tr>
            <tr><td>4</td><td>gradient_boosting_all_fs_simple</td><td>0.5439</td></tr>
            <tr><td>5</td><td>gradient_boosting_all</td><td>0.5436</td></tr>
        </tbody>
    </table>

    <hr>

    <p>
      To squeeze the best performance out of the final model, we can fine-tune it to obtain a better set of hyperparameters. We will generate 48 models from the initial gradient boosting, following a grid search over relevant parameters. Each model will be evaluated on each of the seven benchmarks, and we will obtain the best overall model (same as before).
    </p>

    <h6>Top 5 Model Rankings Santos Small</h6>
        <table width="100%">
            <thead>
                <tr>
                    <th>Rank</th>
                    <th>Model</th>
                    <th>Avg Precision</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>1</td><td>gradient_boosting_ne25_lr0.1_md3_ss1.0_msl1</td><td>0.9719</td></tr>
                <tr><td>2</td><td>gradient_boosting_ne50_lr0.05_md3_ss1.0_msl1</td><td>0.9714</td></tr>
                <tr><td>3</td><td>gradient_boosting_ne50_lr0.05_md3_ss1.0_msl10</td><td>0.9712</td></tr>
                <tr><td>4</td><td>gradient_boosting_ne25_lr0.1_md3_ss1.0_msl10</td><td>0.9706</td></tr>
                <tr><td>5</td><td>gradient_boosting_ne50_lr0.05_md3_ss0.8_msl1</td><td>0.9659</td></tr>
            </tbody>
        </table>

        <h6>Top 5 Model Rankings TUS Small</h6>
        <table width="100%">
            <thead>
                <tr>
                    <th>Rank</th>
                    <th>Model</th>
                    <th>Avg Precision</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>1</td><td>gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10</td><td>0.8958</td></tr>
                <tr><td>2</td><td>gradient_boosting_ne50_lr0.05_md5_ss0.8_msl1</td><td>0.8950</td></tr>
                <tr><td>3</td><td>gradient_boosting_ne50_lr0.05_md5_ss0.8_msl10</td><td>0.8944</td></tr>
                <tr><td>4</td><td>gradient_boosting_ne100_lr0.1_md3_ss0.8_msl10</td><td>0.8936</td></tr>
                <tr><td>5</td><td>gradient_boosting_ne25_lr0.1_md5_ss1.0_msl1</td><td>0.8936</td></tr>
            </tbody>
        </table>

        <h6>Top 5 Model Rankings TUS Big</h6>
        <table width="100%">
            <thead>
                <tr>
                    <th>Rank</th>
                    <th>Model</th>
                    <th>Avg Precision</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>1</td><td>gradient_boosting_ne25_lr0.1_md3_ss1.0_msl10</td><td>0.9381</td></tr>
                <tr><td>2</td><td>gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10</td><td>0.9358</td></tr>
                <tr><td>3</td><td>gradient_boosting_ne50_lr0.05_md3_ss1.0_msl10</td><td>0.9351</td></tr>
                <tr><td>4</td><td>gradient_boosting_ne25_lr0.1_md3_ss1.0_msl1</td><td>0.9350</td></tr>
                <tr><td>5</td><td>gradient_boosting_ne50_lr0.05_md3_ss1.0_msl1</td><td>0.9326</td></tr>
            </tbody>
        </table>

        <h6>Top 5 Model Rankings D3L</h6>
        <table width="100%">
            <thead>
                <tr>
                    <th>Rank</th>
                    <th>Model</th>
                    <th>Avg Precision</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>1</td><td>gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10</td><td>0.8005</td></tr>
                <tr><td>2</td><td>gradient_boosting_ne50_lr0.05_md5_ss0.8_msl1</td><td>0.7993</td></tr>
                <tr><td>3</td><td>gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10</td><td>0.7980</td></tr>
                <tr><td>4</td><td>gradient_boosting_ne50_lr0.1_md3_ss1.0_msl10</td><td>0.7930</td></tr>
                <tr><td>5</td><td>gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10</td><td>0.7843</td></tr>
            </tbody>
        </table>

        <h6>Top 5 Model Rankings Freyja</h6>
        <table width="100%">
            <thead>
                <tr>
                    <th>Rank</th>
                    <th>Model</th>
                    <th>Avg Precision</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>1</td><td>gradient_boosting_ne50_lr0.1_md3_ss1.0_msl1</td><td>0.9624</td></tr>
                <tr><td>2</td><td>gradient_boosting_ne25_lr0.1_md3_ss0.8_msl1</td><td>0.9623</td></tr>
                <tr><td>3</td><td>gradient_boosting_ne25_lr0.1_md3_ss1.0_msl1</td><td>0.9578</td></tr>
                <tr><td>4</td><td>gradient_boosting_ne100_lr0.05_md3_ss0.8_msl1</td><td>0.9576</td></tr>
                <tr><td>5</td><td>gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10</td><td>0.9570</td></tr>
            </tbody>
        </table>

        <h6>Top 5 Model Rankings OM CG</h6>
        <table width="100%">
            <thead>
                <tr>
                    <th>Rank</th>
                    <th>Model</th>
                    <th>Avg Precision</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>1</td><td>gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10</td><td>0.6111</td></tr>
                <tr><td>2</td><td>gradient_boosting_ne100_lr0.1_md3_ss0.8_msl10</td><td>0.6018</td></tr>
                <tr><td>3</td><td>gradient_boosting_ne50_lr0.1_md3_ss0.8_msl1</td><td>0.5990</td></tr>
                <tr><td>4</td><td>gradient_boosting_ne100_lr0.1_md3_ss1.0_msl10</td><td>0.5947</td></tr>
                <tr><td>5</td><td>gradient_boosting_ne100_lr0.1_md3_ss1.0_msl1</td><td>0.5942</td></tr>
            </tbody>
        </table>

        <h6>Top 5 Model Rankings OM CR</h6>
        <table width="100%">
            <thead>
                <tr>
                    <th>Rank</th>
                    <th>Model</th>
                    <th>Avg Precision</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>1</td><td>gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10</td><td>0.6089</td></tr>
                <tr><td>2</td><td>gradient_boosting_ne100_lr0.05_md3_ss0.8_msl1</td><td>0.6034</td></tr>
                <tr><td>3</td><td>gradient_boosting_ne50_lr0.05_md3_ss1.0_msl10</td><td>0.5997</td></tr>
                <tr><td>4</td><td>gradient_boosting_ne50_lr0.1_md3_ss1.0_msl1</td><td>0.5996</td></tr>
                <tr><td>5</td><td>gradient_boosting_ne25_lr0.1_md3_ss1.0_msl10</td><td>0.5994</td></tr>
            </tbody>
        </table>
        <br><br>

        The three best overall models are:

        <table width="100%">
            <thead>
                <tr>
                    <th>Rank</th>
                    <th>Model</th>
                    <th>Avg Precision</th>
                </tr>
            </thead>
            <tbody>
                <tr><td>1</td><td>gradient_boosting_ne100_lr0.05_md3_ss0.8_msl10</td><td>0.8176</td></tr>
                <tr><td>2</td><td>gradient_boosting_ne100_lr0.05_md3_ss1.0_msl10</td><td>0.8163</td></tr>
                <tr><td>3</td><td>gradient_boosting_ne50_lr0.1_md3_ss0.8_msl10</td><td>0.8157</td></tr>
            </tbody>
        </table>



  <p><a href="index.html">&larr; Back to main page</a></p>

  <p style="font-size: small; color: #888;">last updated: 2026/01/07</p>
</div>



</body>
</html>